import streamlit as st
import os
import pickle
import hashlib
import json
import time
from pathlib import Path
from dotenv import load_dotenv
import glob
# LangChain imports
from langchain_groq import ChatGroq
from langchain_community.llms import Ollama
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever

# Load environment
load_dotenv()

# Configuration
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
SEMANTIC_WEIGHT = 0.7
KEYWORD_WEIGHT = 0.3
NUM_RESULTS = 8

# Available Models
AVAILABLE_MODELS = {
    "Local Models (Ollama)": {
        "llama3.2:3b": "llama3.2:3b",
        "llama3.2:1b": "llama3.2:1b",
        "llama3.1:8b": "llama3.1:8b",
        "mistral:7b": "mistral:7b",
        "codellama:7b": "codellama:7b"
    },
    "API Models (Groq)": {
        "Llama 3.1 8B": "llama-3.1-8b-instant",
        "Llama 3.1 70B": "llama-3.1-70b-versatile",
        "Mixtral 8x7B": "mixtral-8x7b-32768",
        "Gemma 2 9B": "gemma2-9b-it",
        "GPT-OSS 120B": "openai/gpt-oss-120b"
    }
}

def check_ollama_connection():
    """Check if Ollama is running and accessible"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def get_available_ollama_models():
    """Get list of available Ollama models"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models_data = response.json()
            return [model['name'] for model in models_data.get('models', [])]
        return []
    except:
        return []

def initialize_llm(model_provider, model_name, groq_api_key=None):
    """Initialize the appropriate LLM based on provider"""
    try:
        if model_provider == "Local Models (Ollama)":
            if not check_ollama_connection():
                st.error("âŒ Ollama is not running! Please start Ollama service.")
                return None

            available_models = get_available_ollama_models()
            if model_name not in available_models:
                st.warning(f"âš ï¸ Model '{model_name}' not found in Ollama.")
                st.info("To install: `ollama pull " + model_name + "`")
                return None

            return Ollama(model=model_name, base_url="http://localhost:11434", temperature=0.1)

        elif model_provider == "API Models (Groq)":
            if not groq_api_key:
                st.error("âŒ GROQ_API_KEY not found!")
                return None

            return ChatGroq(groq_api_key=groq_api_key, model_name=model_name, temperature=0)

        return None
    except Exception as e:
        st.error(f"âŒ Error initializing LLM: {e}")
        return None

# Try to get API key from secrets first, then env
try:
    groq_api_key = st.secrets.get('GROQ_API_KEY', os.environ.get('GROQ_API_KEY'))
except:
    groq_api_key = os.environ.get('GROQ_API_KEY')

def clean_text(text):
    """Clean and validate text content"""
    if not text or not isinstance(text, str):
        return ""
    cleaned = text.encode('utf-8', errors='ignore').decode('utf-8').strip()
    return ' '.join(cleaned.split())

def get_all_pdfs(folder_path):
    """Get all PDFs from specified folder and subfolders"""
    pdf_patterns = [
        os.path.join(folder_path, "**/*.pdf"),
        os.path.join(folder_path, "*.pdf")
    ]
    all_pdfs = []
    for pattern in pdf_patterns:
        all_pdfs.extend(glob.glob(pattern, recursive=True))
    return [Path(pdf) for pdf in set(all_pdfs) if os.path.exists(pdf)]

def scan_folder_for_databases(folder_path):
    """Scan a folder for vector databases"""
    databases = []
    try:
        if not os.path.exists(folder_path):
            return databases

        vector_db_path = os.path.join(folder_path, "vector_databases")
        if os.path.exists(vector_db_path):
            for item in os.listdir(vector_db_path):
                if item.startswith('db_'):
                    db_path = os.path.join(vector_db_path, item)
                    metadata_path = os.path.join(db_path, 'metadata.json')
                    if os.path.exists(metadata_path):
                        try:
                            with open(metadata_path, 'r') as f:
                                metadata = json.load(f)
                            databases.append({
                                'name': f"{metadata.get('embedding_model', 'Unknown')} - {metadata.get('total_pdfs', 0)} PDFs",
                                'path': vector_db_path,
                                'metadata': metadata,
                                'db_folder': item
                            })
                        except:
                            continue

    except Exception:
        pass

    return databases

def load_existing_database(db_path, db_folder):
    """Load an existing vector database"""
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=f"sentence-transformers/{EMBEDDING_MODEL}",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        base_path = os.path.join(db_path, db_folder)
        faiss_path = os.path.join(base_path, "faiss_index")
        bm25_path = os.path.join(base_path, "bm25_retriever.pkl")
        documents_path = os.path.join(base_path, "documents.pkl")
        metadata_path = os.path.join(base_path, "metadata.json")

        vector_store = FAISS.load_local(faiss_path, embeddings, allow_dangerous_deserialization=True)

        with open(bm25_path, 'rb') as f:
            bm25_retriever = pickle.load(f)

        with open(documents_path, 'rb') as f:
            documents = pickle.load(f)

        with open(metadata_path, 'r') as f:
            metadata = json.load(f)

        return vector_store, bm25_retriever, documents, embeddings, metadata

    except Exception as e:
        st.error(f"Error loading database: {e}")
        return None

def create_new_database(pdf_files, save_folder):
    """Create new vector database from PDF files"""
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=f"sentence-transformers/{EMBEDDING_MODEL}",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        with st.spinner(f"ğŸ”„ Processing {len(pdf_files)} PDFs..."):
            documents = []
            progress_bar = st.progress(0)

            for i, pdf_path in enumerate(pdf_files):
                try:
                    st.text(f"ğŸ“„ Loading {pdf_path.name} ({i+1}/{len(pdf_files)})...")
                    loader = PyPDFLoader(str(pdf_path))
                    pdf_docs = loader.load()

                    valid_docs = []
                    for doc in pdf_docs:
                        cleaned_content = clean_text(doc.page_content)
                        if len(cleaned_content) > 20:
                            doc.page_content = cleaned_content
                            doc.metadata['source_file'] = pdf_path.name
                            doc.metadata['full_path'] = str(pdf_path)
                            valid_docs.append(doc)

                    documents.extend(valid_docs)
                    progress_bar.progress((i + 1) / len(pdf_files))

                except Exception as e:
                    st.warning(f"âš ï¸ Failed to load {pdf_path.name}: {str(e)}")

            st.text("âœ‚ï¸ Splitting documents...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP
            )
            final_documents = text_splitter.split_documents(documents)

            st.text("ğŸ”¢ Creating vector database...")
            vector_store = FAISS.from_documents(final_documents, embeddings)

            st.text("ğŸ” Creating keyword index...")
            bm25_retriever = BM25Retriever.from_documents(final_documents)
            bm25_retriever.k = NUM_RESULTS

            st.text("ğŸ’¾ Saving database...")
            config_str = f"{sorted([str(p) for p in pdf_files])}_{EMBEDDING_MODEL}_{CHUNK_SIZE}_{CHUNK_OVERLAP}"
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:12]

            os.makedirs(save_folder, exist_ok=True)
            db_folder = f"db_{config_hash}"
            db_path = os.path.join(save_folder, db_folder)
            os.makedirs(db_path, exist_ok=True)

            vector_store.save_local(os.path.join(db_path, "faiss_index"))

            with open(os.path.join(db_path, "bm25_retriever.pkl"), 'wb') as f:
                pickle.dump(bm25_retriever, f)

            with open(os.path.join(db_path, "documents.pkl"), 'wb') as f:
                pickle.dump(final_documents, f)

            metadata = {
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'embedding_model': EMBEDDING_MODEL,
                'chunk_size': CHUNK_SIZE,
                'chunk_overlap': CHUNK_OVERLAP,
                'total_pdfs': len(pdf_files),
                'total_chunks': len(final_documents),
                'pdf_files': [str(p) for p in pdf_files],
                'config_hash': config_hash
            }

            with open(os.path.join(db_path, "metadata.json"), 'w') as f:
                json.dump(metadata, f, indent=2)

            progress_bar.progress(1.0)
            st.success(f"âœ… Database created successfully!")

            return vector_store, bm25_retriever, final_documents, embeddings, metadata

    except Exception as e:
        st.error(f"âŒ Error creating database: {e}")
        return None

def process_answer_with_citations(answer, context_docs):
    """Add inline citations to the answer text"""
    import re

    citations = {}
    for i, doc in enumerate(context_docs):
        source_file = doc.metadata.get('source_file', 'Unknown')
        page_num = doc.metadata.get('page', 'Unknown')
        full_path = doc.metadata.get('full_path', '')

        citation_key = f"[{i+1}]"
        if full_path and os.path.exists(full_path):
            citation_link = f"""<sup><a href="file:///{full_path}" target="_blank" class="citation-link" title="Open {source_file} - Page {page_num}">[{i+1}]</a></sup>"""
        else:
            citation_link = f"""<sup class="citation-unavailable" title="{source_file} - Page {page_num}">[{i+1}]</sup>"""

        citations[citation_key] = citation_link

    processed_answer = answer

    patterns_to_cite = [
        r'(Item[â€‘\-]\s*No\.\s*[\d\-â€‘]+[\w\-â€‘]*)',
        r'(Rs\.?\s*[\d,]+\.?\d*)',
        r'(Chapter\s*[\d]+)',
        r'(\d+\.?\d*\s*%)',
        r'(Rule\s*[\d\.]+)',
        r'(\d+â€³\s*Ã—\s*\d+â€³)',
        r'(Sq\s*[mft]+)',
        r'(MRS|CSR|Market[â€‘\-]Rate\s*System)',
    ]

    citation_usage = [False] * len(context_docs)

    for pattern in patterns_to_cite:
        matches = list(re.finditer(pattern, processed_answer, re.IGNORECASE))
        for match in reversed(matches):
            match_text = match.group(1).lower()
            best_citation_idx = None
            best_score = 0

            for i, doc in enumerate(context_docs):
                if citation_usage[i]:
                    continue

                doc_content = doc.page_content.lower()
                score = 0
                if 'item' in match_text and 'item' in doc_content:
                    score += 3
                if 'chapter' in match_text and 'chapter' in doc_content:
                    score += 3
                if 'rs' in match_text and 'rs' in doc_content:
                    score += 2
                if any(word in doc_content for word in match_text.split()):
                    score += 1

                if score > best_score:
                    best_score = score
                    best_citation_idx = i

            if best_citation_idx is not None and best_score > 0:
                citation_link = citations[f"[{best_citation_idx+1}]"]
                citation_usage[best_citation_idx] = True

                end_pos = match.end()
                processed_answer = (processed_answer[:end_pos] +
                                  citation_link +
                                  processed_answer[end_pos:])

    return processed_answer

def create_hybrid_retriever(vector_store, bm25_retriever):
    """Create hybrid retriever combining semantic and keyword search"""
    vector_retriever = vector_store.as_retriever(search_kwargs={"k": NUM_RESULTS})

    ensemble_retriever = EnsembleRetriever(
        retrievers=[vector_retriever, bm25_retriever],
        weights=[SEMANTIC_WEIGHT, KEYWORD_WEIGHT]
    )

    return ensemble_retriever

# Page Configuration
st.set_page_config(
    page_title="DocuMind AI",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Ultra Modern CSS with Complete Redesign
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500;600&display=swap');

    * {
        font-family: 'Poppins', sans-serif;
        transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    }

    /* Dark Modern Background */
    .stApp {
        background: linear-gradient(135deg, #0f172a 0%, #1e293b 50%, #334155 100%);
    }

    .main {
        background: transparent;
        padding: 0 !important;
    }

    .main > div {
        background: transparent;
        padding: 0 !important;
    }

    /* Modern Header Bar */
    .top-bar {
        background: rgba(255, 255, 255, 0.05);
        backdrop-filter: blur(20px);
        border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        padding: 1.5rem 3rem;
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 2rem;
    }

    .logo-section {
        display: flex;
        align-items: center;
        gap: 1rem;
    }

    .logo-icon {
        font-size: 2.5rem;
        background: linear-gradient(135deg, #6366f1, #a855f7);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
    }

    .logo-text {
        color: white;
        font-size: 1.75rem;
        font-weight: 700;
        letter-spacing: -0.5px;
    }

    .status-badge {
        background: linear-gradient(135deg, #10b981, #059669);
        color: white;
        padding: 0.5rem 1.25rem;
        border-radius: 50px;
        font-size: 0.875rem;
        font-weight: 600;
        display: inline-flex;
        align-items: center;
        gap: 0.5rem;
    }

    /* Container */
    .content-container {
        max-width: 1400px;
        margin: 0 auto;
        padding: 0 2rem 3rem 2rem;
    }

    /* Modern Cards */
    .modern-card {
        background: rgba(255, 255, 255, 0.05);
        backdrop-filter: blur(20px);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 24px;
        padding: 2.5rem;
        margin-bottom: 2rem;
        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
    }

    .modern-card:hover {
        transform: translateY(-4px);
        border-color: rgba(99, 102, 241, 0.5);
        box-shadow: 0 30px 80px rgba(99, 102, 241, 0.2);
    }

    .card-title {
        color: white;
        font-size: 1.5rem;
        font-weight: 700;
        margin-bottom: 1.5rem;
        display: flex;
        align-items: center;
        gap: 0.75rem;
    }

    .card-title-icon {
        font-size: 1.75rem;
    }

    /* Glassmorphism Panels */
    .glass-panel {
        background: rgba(255, 255, 255, 0.03);
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.08);
        border-radius: 16px;
        padding: 1.5rem;
        margin-bottom: 1.5rem;
    }

    /* Modern Tabs */
    .stTabs [data-baseweb="tab-list"] {
        background: rgba(255, 255, 255, 0.05);
        backdrop-filter: blur(10px);
        border-radius: 16px;
        padding: 0.5rem;
        gap: 0.5rem;
        border: 1px solid rgba(255, 255, 255, 0.1);
    }

    .stTabs [data-baseweb="tab"] {
        background: transparent;
        border-radius: 12px;
        color: rgba(255, 255, 255, 0.6) !important;
        font-weight: 600;
        padding: 0.875rem 1.5rem;
        border: none;
    }

    .stTabs [data-baseweb="tab"]:hover {
        background: rgba(255, 255, 255, 0.05);
        color: rgba(255, 255, 255, 0.9) !important;
    }

    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #6366f1, #a855f7) !important;
        color: white !important;
    }

    /* Text Colors */
    p, span, div, label {
        color: rgba(255, 255, 255, 0.9) !important;
    }

    h1, h2, h3, h4, h5, h6 {
        color: white !important;
        font-weight: 700;
    }

    /* Modern Buttons */
    .stButton > button {
        background: linear-gradient(135deg, #6366f1, #a855f7);
        color: white !important;
        border: none;
        border-radius: 12px;
        padding: 0.875rem 2rem;
        font-weight: 600;
        font-size: 0.95rem;
        box-shadow: 0 8px 20px rgba(99, 102, 241, 0.4);
        letter-spacing: 0.3px;
    }

    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 12px 30px rgba(99, 102, 241, 0.5);
        background: linear-gradient(135deg, #a855f7, #6366f1);
    }

    /* Input Fields */
    .stTextInput > div > div > input,
    .stTextArea > div > div > textarea {
        background: rgba(255, 255, 255, 0.05) !important;
        border: 1px solid rgba(255, 255, 255, 0.1) !important;
        border-radius: 12px !important;
        color: white !important;
        padding: 0.875rem 1rem !important;
    }

    .stTextInput > div > div > input:focus,
    .stTextArea > div > div > textarea:focus {
        border-color: #6366f1 !important;
        box-shadow: 0 0 0 3px rgba(99, 102, 241, 0.2) !important;
    }

    .stTextInput > div > div > input::placeholder,
    .stTextArea > div > div > textarea::placeholder {
        color: rgba(255, 255, 255, 0.4) !important;
    }

    /* Select & Radio */
    .stSelectbox > div > div,
    .stRadio > div {
        background: rgba(255, 255, 255, 0.05) !important;
        border: 1px solid rgba(255, 255, 255, 0.1) !important;
        border-radius: 12px !important;
        color: white !important;
    }

    .stSelectbox label,
    .stRadio label {
        color: rgba(255, 255, 255, 0.9) !important;
        font-weight: 500;
    }

    /* Metrics */
    [data-testid="metric-container"] {
        background: rgba(255, 255, 255, 0.05);
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 16px;
        padding: 1.5rem;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
    }

    [data-testid="metric-container"]:hover {
        transform: translateY(-4px);
        border-color: rgba(99, 102, 241, 0.5);
    }

    [data-testid="metric-container"] label {
        color: rgba(255, 255, 255, 0.6) !important;
        font-weight: 600;
        font-size: 0.75rem;
        text-transform: uppercase;
        letter-spacing: 1px;
    }

    [data-testid="metric-container"] [data-testid="stMetricValue"] {
        color: #6366f1 !important;
        font-weight: 800;
        font-size: 2rem;
    }

    /* Chat Messages */
    .stChatMessage {
        background: rgba(255, 255, 255, 0.05);
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 16px;
        padding: 1.5rem;
        margin: 1rem 0;
    }

    .stChatMessage p,
    .stChatMessage div,
    .stChatMessage span {
        color: rgba(255, 255, 255, 0.9) !important;
    }

    /* Chat Input */
    [data-testid="stChatInput"] {
        background: rgba(255, 255, 255, 0.05) !important;
        backdrop-filter: blur(10px);
        border: 1px solid rgba(255, 255, 255, 0.1) !important;
        border-radius: 16px !important;
    }

    [data-testid="stChatInput"] input {
        background: transparent !important;
        color: white !important;
        border: none !important;
    }

    /* Success/Info/Warning Boxes */
    .stSuccess, .stInfo, .stWarning, .stError {
        background: rgba(255, 255, 255, 0.05) !important;
        backdrop-filter: blur(10px);
        border-radius: 12px;
        border-left: 4px solid;
    }

    .stSuccess {
        border-color: #10b981;
    }

    .stInfo {
        border-color: #6366f1;
    }

    .stWarning {
        border-color: #f59e0b;
    }

    .stError {
        border-color: #ef4444;
    }

    /* Expander */
    .streamlit-expanderHeader {
        background: rgba(255, 255, 255, 0.05) !important;
        backdrop-filter: blur(10px);
        border-radius: 12px !important;
        color: white !important;
        font-weight: 600;
    }

    .streamlit-expanderContent {
        background: rgba(255, 255, 255, 0.03) !important;
        border-radius: 0 0 12px 12px !important;
    }

    /* Code Blocks */
    code, pre {
        background: rgba(0, 0, 0, 0.3) !important;
        color: #a855f7 !important;
        border-radius: 8px;
        padding: 0.25rem 0.5rem;
        font-family: 'JetBrains Mono', monospace;
    }

    /* Progress Bar */
    .stProgress > div > div {
        background: linear-gradient(90deg, #6366f1, #a855f7);
    }

    /* Citations */
    .citation-link {
        background: rgba(99, 102, 241, 0.2);
        color: #a78bfa !important;
        padding: 2px 8px;
        border-radius: 6px;
        text-decoration: none !important;
        font-weight: 600;
        border: 1px solid rgba(99, 102, 241, 0.3);
    }

    .citation-link:hover {
        background: linear-gradient(135deg, #6366f1, #a855f7);
        color: white !important;
        transform: translateY(-1px);
    }

    /* Scrollbar */
    ::-webkit-scrollbar {
        width: 10px;
        height: 10px;
    }

    ::-webkit-scrollbar-track {
        background: rgba(255, 255, 255, 0.05);
    }

    ::-webkit-scrollbar-thumb {
        background: linear-gradient(135deg, #6366f1, #a855f7);
        border-radius: 5px;
    }

    ::-webkit-scrollbar-thumb:hover {
        background: linear-gradient(135deg, #a855f7, #6366f1);
    }

    /* Answer Content */
    .answer-content {
        color: rgba(255, 255, 255, 0.9) !important;
        line-height: 1.8;
    }

    .answer-content h1,
    .answer-content h2,
    .answer-content h3 {
        color: #a78bfa !important;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }

    .answer-content table {
        width: 100%;
        border-collapse: collapse;
        margin: 1.5rem 0;
    }

    .answer-content th,
    .answer-content td {
        border: 1px solid rgba(255, 255, 255, 0.1);
        padding: 0.875rem 1rem;
        text-align: left;
        color: rgba(255, 255, 255, 0.9) !important;
    }

    .answer-content th {
        background: rgba(99, 102, 241, 0.2);
        font-weight: 600;
        color: white !important;
    }

    .answer-content ul,
    .answer-content ol {
        margin: 1rem 0;
        padding-left: 2rem;
    }

    .answer-content li {
        margin: 0.5rem 0;
        color: rgba(255, 255, 255, 0.9) !important;
    }

    /* Hide Streamlit Branding */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# Clear Streamlit cache on startup (force fresh scan)
st.cache_data.clear()
st.cache_resource.clear()

# Initialize Session State
if 'database_loaded' not in st.session_state:
    st.session_state.database_loaded = False
if 'llm_configured' not in st.session_state:
    st.session_state.llm_configured = False
if "messages" not in st.session_state:
    st.session_state.messages = []

# Sidebar File Explorer
with st.sidebar:
    st.markdown("""
    <div style="text-align: center; padding: 1.5rem 0; border-bottom: 1px solid rgba(255,255,255,0.1);">
        <h2 style="color: white; margin: 0; font-size: 1.5rem;">ğŸ§  DocuMind AI</h2>
        <p style="color: rgba(255,255,255,0.5); margin: 0.5rem 0 0 0; font-size: 0.875rem;">Document Intelligence</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("### ğŸ“ Loaded Documents")
    
    if st.session_state.database_loaded and 'metadata' in st.session_state:
        metadata = st.session_state.metadata
        
        # Display document tree
        st.markdown(f"""
        <div style="background: rgba(255,255,255,0.05); padding: 1rem; border-radius: 12px; margin-top: 1rem;">
            <p style="color: rgba(255,255,255,0.6); margin: 0 0 0.5rem 0; font-size: 0.75rem;">DATABASE INFO</p>
            <p style="color: white; margin: 0; font-weight: 600;">ğŸ“Š {metadata.get('total_pdfs', 0)} Documents</p>
            <p style="color: rgba(255,255,255,0.7); margin: 0.25rem 0 0 0; font-size: 0.875rem;">
                {metadata.get('total_chunks', 0):,} chunks indexed
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("#### ğŸ“„ Files")
        
        # Display each PDF file
        pdf_files = metadata.get('pdf_files', [])
        for i, pdf_path in enumerate(pdf_files[:20], 1):  # Show max 20 files
            file_name = os.path.basename(pdf_path)
            
            # Create expandable file item
            with st.expander(f"ğŸ“„ {file_name[:30]}{'...' if len(file_name) > 30 else ''}", expanded=False):
                st.markdown(f"""
                <div style="font-size: 0.75rem; color: rgba(255,255,255,0.6);">
                    <p style="margin: 0.25rem 0;"><strong>File:</strong> {file_name}</p>
                    <p style="margin: 0.25rem 0;"><strong>Path:</strong> {pdf_path[:50]}...</p>
                </div>
                """, unsafe_allow_html=True)
        
        if len(pdf_files) > 20:
            st.markdown(f"<p style='color: rgba(255,255,255,0.5); font-size: 0.875rem;'>+ {len(pdf_files) - 20} more files</p>", unsafe_allow_html=True)
    else:
        st.markdown("""
        <div style="text-align: center; padding: 2rem 1rem; color: rgba(255,255,255,0.4);">
            <p style="font-size: 2rem; margin: 0;">ğŸ“­</p>
            <p style="margin: 0.5rem 0 0 0; font-size: 0.875rem;">No documents loaded</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Status indicators
    st.markdown("---")
    st.markdown("### âš™ï¸ System Status")
    
    db_status = "ğŸŸ¢ Active" if st.session_state.database_loaded else "ğŸ”´ Not Loaded"
    ai_status = "ğŸŸ¢ Ready" if st.session_state.llm_configured else "ğŸ”´ Not Configured"
    
    st.markdown(f"""
    <div style="font-size: 0.875rem;">
        <p style="margin: 0.5rem 0; color: rgba(255,255,255,0.8);">
            <strong>Database:</strong> {db_status}
        </p>
        <p style="margin: 0.5rem 0; color: rgba(255,255,255,0.8);">
            <strong>AI Model:</strong> {ai_status}
        </p>
    </div>
    """, unsafe_allow_html=True)

# Modern Header Bar
st.markdown("""
<div class="top-bar">
    <div class="logo-section">
        <div class="logo-icon">ğŸ§ </div>
        <div class="logo-text">DocuMind AI</div>
    </div>
    <div class="status-badge">
        <span>â—</span> System Ready
    </div>
</div>
""", unsafe_allow_html=True)

# Content Container
st.markdown('<div class="content-container">', unsafe_allow_html=True)

# Main Tabs Navigation
tab1, tab2, tab3 = st.tabs(["ğŸ  Dashboard", "âš™ï¸ Configuration", "ğŸ’¬ Chat"])

# TAB 1: DASHBOARD
with tab1:
    st.markdown('<div class="card-title"><span class="card-title-icon">ğŸ“Š</span>System Overview</div>', unsafe_allow_html=True)

    if st.session_state.database_loaded and st.session_state.llm_configured:
        metadata = st.session_state.metadata

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("ğŸ“„ Documents", metadata['total_pdfs'])
        with col2:
            st.metric("ğŸ“ Text Chunks", f"{metadata['total_chunks']:,}")
        with col3:
            st.metric("ğŸ¤– AI Model", st.session_state.selected_model.split(':')[0][:15])
        with col4:
            st.metric("ğŸ” Search Mode", "Hybrid")

        st.markdown('<div style="margin-top: 2rem;"></div>', unsafe_allow_html=True)

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            <div class="glass-panel">
                <h4 style="color: white; margin-bottom: 1rem;">ğŸ“Š Database Details</h4>
                <p style="color: rgba(255,255,255,0.7); margin: 0.5rem 0;">
                    <strong>Embedding Model:</strong> {}</p>
                <p style="color: rgba(255,255,255,0.7); margin: 0.5rem 0;">
                    <strong>Created:</strong> {}</p>
                <p style="color: rgba(255,255,255,0.7); margin: 0.5rem 0;">
                    <strong>Chunk Size:</strong> {} tokens</p>
            </div>
            """.format(
                metadata['embedding_model'],
                metadata['created_at'],
                metadata['chunk_size']
            ), unsafe_allow_html=True)

        with col2:
            st.markdown("""
            <div class="glass-panel">
                <h4 style="color: white; margin-bottom: 1rem;">ğŸ¤– AI Configuration</h4>
                <p style="color: rgba(255,255,255,0.7); margin: 0.5rem 0;">
                    <strong>Provider:</strong> {}</p>
                <p style="color: rgba(255,255,255,0.7); margin: 0.5rem 0;">
                    <strong>Model:</strong> {}</p>
                <p style="color: rgba(255,255,255,0.7); margin: 0.5rem 0;">
                    <strong>Status:</strong> <span style="color: #10b981;">â— Active</span></p>
            </div>
            """.format(
                "Local (Ollama)" if st.session_state.model_provider == "Local Models (Ollama)" else "Cloud (Groq)",
                st.session_state.selected_model
            ), unsafe_allow_html=True)

    else:
        st.markdown("""
        <div class="modern-card">
            <h3 style="color: white; text-align: center; margin-bottom: 1rem;">Welcome to DocuMind AI</h3>
            <p style="color: rgba(255,255,255,0.7); text-align: center; font-size: 1.1rem;">
                Please configure your database and AI model in the Configuration tab to get started.
            </p>
        </div>
        """, unsafe_allow_html=True)

# TAB 2: CONFIGURATION
with tab2:
    st.markdown('<div class="card-title"><span class="card-title-icon">âš™ï¸</span>Quick Setup</div>', unsafe_allow_html=True)
    
    # Simplified single-step data loading
    st.markdown("""
    <div class="glass-panel">
        <h4 style="color: white; margin-bottom: 1rem;">ğŸ“ Load Your Documents</h4>
        <p style="color: rgba(255,255,255,0.7); margin-bottom: 1rem;">
            Upload PDF files or select from existing databases
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Initialize session state
    if 'scanned_databases' not in st.session_state:
        st.session_state.scanned_databases = []
    if 'found_pdfs' not in st.session_state:
        st.session_state.found_pdfs = []
    
    # Auto-scan for existing databases
    if not st.session_state.scanned_databases:
        databases = scan_folder_for_databases(".")
        st.session_state.scanned_databases = databases
    
    # Show existing databases first
    if st.session_state.scanned_databases:
        st.markdown("### ğŸ“š Existing Databases")
        
        selected_db = st.selectbox(
            "Select Database",
            range(len(st.session_state.scanned_databases)),
            format_func=lambda x: st.session_state.scanned_databases[x]['name']
        )
        
        col1, col2 = st.columns([1, 3])
        with col1:
            if st.button("ğŸš€ Load Database", key="load_db", use_container_width=True):
                with st.spinner("Loading database..."):
                    db_info = st.session_state.scanned_databases[selected_db]
                    result = load_existing_database(db_info['path'], db_info['db_folder'])
                    
                    if result:
                        vector_store, bm25_retriever, documents, embeddings, metadata = result
                        st.session_state.vector_store = vector_store
                        st.session_state.bm25_retriever = bm25_retriever
                        st.session_state.documents = documents
                        st.session_state.embeddings = embeddings
                        st.session_state.metadata = metadata
                        st.session_state.database_loaded = True
                        st.success("âœ… Database loaded!")
                        st.rerun()
        
        st.markdown("---")
    
    # Upload new files section
    st.markdown("### ğŸ“¤ Upload New Documents")
    
    uploaded_files = st.file_uploader(
        "Drag and drop PDF files here",
        type=['pdf'],
        accept_multiple_files=True,
        key="pdf_uploader"
    )
    
    if uploaded_files:
        if st.button("ğŸš€ Process & Load Files", key="process_load", use_container_width=True):
            with st.spinner("Processing files..."):
                import tempfile
                temp_dir = tempfile.mkdtemp()
                pdf_paths = []
                
                for uploaded_file in uploaded_files:
                    temp_path = os.path.join(temp_dir, uploaded_file.name)
                    with open(temp_path, 'wb') as f:
                        f.write(uploaded_file.getbuffer())
                    pdf_paths.append(Path(temp_path))
                
                # Create database immediately
                result = create_new_database(pdf_paths, "vector_databases")
                
                if result:
                    vector_store, bm25_retriever, documents, embeddings, metadata = result
                    st.session_state.vector_store = vector_store
                    st.session_state.bm25_retriever = bm25_retriever
                    st.session_state.documents = documents
                    st.session_state.embeddings = embeddings
                    st.session_state.metadata = metadata
                    st.session_state.database_loaded = True
                    st.session_state.scanned_databases = []  # Reset to rescan
                    st.success("âœ… Files processed and loaded!")
                    st.rerun()
    
    st.markdown("---")
    
    # AI Model Configuration
    st.markdown('<div class="card-title"><span class="card-title-icon">ğŸ¤–</span>AI Model</div>', unsafe_allow_html=True)

        if db_option == "Load Existing Database":
            st.markdown('<div style="margin-top: 2rem;"></div>', unsafe_allow_html=True)

            folder_path = st.text_input("ğŸ“ Database Folder Path", value="vector_databases")

            if st.button("ğŸ” Scan for Databases", key="scan_db"):
                databases = scan_folder_for_databases(".")
                st.session_state.scanned_databases = databases
                if databases:
                    st.success(f"âœ… Found {len(databases)} database(s)")
                else:
                    st.warning("âš ï¸ No databases found")

            # Show database selection if databases are scanned
            if st.session_state.scanned_databases:
                st.markdown('<div style="margin-top: 1.5rem;"></div>', unsafe_allow_html=True)

                selected_db = st.selectbox(
                    "Select Database",
                    range(len(st.session_state.scanned_databases)),
                    format_func=lambda x: st.session_state.scanned_databases[x]['name']
                )

                if st.button("ğŸš€ Load Database", key="load_db"):
                    with st.spinner("Loading database..."):
                        db_info = st.session_state.scanned_databases[selected_db]
                        result = load_existing_database(db_info['path'], db_info['db_folder'])

                        if result:
                            vector_store, bm25_retriever, documents, embeddings, metadata = result
                            st.session_state.vector_store = vector_store
                            st.session_state.bm25_retriever = bm25_retriever
                            st.session_state.documents = documents
                            st.session_state.embeddings = embeddings
                            st.session_state.metadata = metadata
                            st.session_state.database_loaded = True
                            st.success("âœ… Database loaded successfully! Switch to AI Model tab to configure.")
                            st.balloons()

        else:  # Create New Database
            st.markdown('<div style="margin-top: 2rem;"></div>', unsafe_allow_html=True)

            # Initialize session state for PDFs
            if 'found_pdfs' not in st.session_state:
                st.session_state.found_pdfs = []
            if 'uploaded_files' not in st.session_state:
                st.session_state.uploaded_files = []

            st.markdown("""
            <div class="glass-panel">
                <h4 style="color: white; margin-bottom: 1rem;">ğŸ“‚ Select PDF Source</h4>
                <p style="color: rgba(255,255,255,0.7); margin-bottom: 1rem;">
                    Choose how to provide your PDF documents
                </p>
            </div>
            """, unsafe_allow_html=True)

            # Source selection
            source_option = st.radio(
                "ğŸ“ PDF Source",
                ["ğŸ“¤ Upload Files (Drag & Drop)", "ğŸ“‚ Use Server Folder"],
                horizontal=True,
                help="Choose to upload files from your computer or use files from server"
            )

            if source_option == "ğŸ“¤ Upload Files (Drag & Drop)":
                st.markdown('<div style="margin-top: 1.5rem;"></div>', unsafe_allow_html=True)
                
                st.markdown("""
                <div class="glass-panel" style="background: rgba(99, 102, 241, 0.1); border-color: rgba(99, 102, 241, 0.3);">
                    <h4 style="color: #a78bfa; margin-bottom: 0.5rem;">ğŸ“¤ Upload Your PDF Files</h4>
                    <p style="color: rgba(255,255,255,0.8); margin: 0.5rem 0;">
                        Drag and drop PDF files below or click to browse
                    </p>
                    <p style="color: rgba(255,255,255,0.6); margin: 0; font-size: 0.9rem;">
                        You can upload multiple files at once
                    </p>
                </div>
                """, unsafe_allow_html=True)

                uploaded_files = st.file_uploader(
                    "Choose PDF files",
                    type=['pdf'],
                    accept_multiple_files=True,
                    key="pdf_uploader",
                    help="Select one or more PDF files from your computer"
                )

                save_folder = st.text_input(
                    "ğŸ’¾ Database Save Location",
                    value="vector_databases",
                    help="Where to save the vector database"
                )

                if uploaded_files:
                    st.markdown('<div style="margin-top: 1.5rem;"></div>', unsafe_allow_html=True)
                    
                    # Show uploaded files preview
                    with st.expander(f"ğŸ“„ Uploaded Files ({len(uploaded_files)})", expanded=True):
                        for i, uploaded_file in enumerate(uploaded_files, 1):
                            file_size = uploaded_file.size / 1024  # KB
                            st.markdown(f"**{i}.** `{uploaded_file.name}` ({file_size:.1f} KB)")

                    # Process uploaded files
                    if st.button("ğŸ”„ Process Uploaded Files", key="process_uploads", width="stretch"):
                        with st.spinner("Saving uploaded files..."):
                            import tempfile
                            temp_dir = tempfile.mkdtemp()
                            pdf_paths = []
                            
                            try:
                                for uploaded_file in uploaded_files:
                                    # Save uploaded file to temporary location
                                    temp_path = os.path.join(temp_dir, uploaded_file.name)
                                    with open(temp_path, 'wb') as f:
                                        f.write(uploaded_file.getbuffer())
                                    pdf_paths.append(Path(temp_path))
                                
                                st.session_state.found_pdfs = pdf_paths
                                st.session_state.pdf_folder = temp_dir
                                st.session_state.save_folder = save_folder
                                st.session_state.uploaded_files = uploaded_files
                                
                                st.success(f"âœ… {len(pdf_paths)} file(s) ready for processing!")
                                
                            except Exception as e:
                                st.error(f"âŒ Error processing files: {e}")

            else:  # Use Server Folder
                st.markdown('<div style="margin-top: 1.5rem;"></div>', unsafe_allow_html=True)

                # Get current directory and list available folders
                current_dir = os.getcwd()
                
                # Get list of directories in workspace
                available_folders = ["Data"]  # Default folder
                try:
                    for item in os.listdir(current_dir):
                        full_path = os.path.join(current_dir, item)
                        if os.path.isdir(full_path) and not item.startswith('.') and item not in ['vector_databases', 'vector_databases_azhar', '__pycache__', 'assets']:
                            if item not in available_folders:
                                available_folders.append(item)
                except:
                    pass

                col1, col2 = st.columns([2, 1])
                
                with col1:
                    # Folder selection dropdown
                    selected_folder = st.selectbox(
                        "ğŸ“ Select Folder",
                        available_folders,
                        help="Choose a folder from the server"
                    )
                    
                    # Option to enter custom path
                    use_custom = st.checkbox("Use custom folder path", key="use_custom_path")
                    
                    if use_custom:
                        pdf_folder = st.text_input(
                            "Custom Folder Path",
                            value=selected_folder,
                            placeholder="/path/to/your/pdfs",
                            help="Enter the full path to your PDF folder"
                        )
                    else:
                        pdf_folder = selected_folder

                with col2:
                    save_folder = st.text_input(
                        "ğŸ’¾ Database Save Location",
                        value="vector_databases",
                        help="Where to save the vector database"
                    )

                st.markdown('<div style="margin-top: 1.5rem;"></div>', unsafe_allow_html=True)

                if st.button("ğŸ” Scan Selected Folder", key="check_pdfs", width="stretch"):
                    if os.path.exists(pdf_folder):
                        with st.spinner(f"Scanning {pdf_folder} for PDF files..."):
                            pdf_files = get_all_pdfs(pdf_folder)
                            st.session_state.found_pdfs = pdf_files
                            st.session_state.pdf_folder = pdf_folder
                            st.session_state.save_folder = save_folder
                            
                            if pdf_files:
                                st.success(f"âœ… Found {len(pdf_files)} PDF file(s) in {pdf_folder}")
                                
                                # Show preview of found files
                                with st.expander("ğŸ“„ View Found PDFs", expanded=True):
                                    for i, pdf in enumerate(pdf_files[:10], 1):  # Show first 10
                                        st.markdown(f"**{i}.** `{pdf.name}` ({pdf.parent})")
                                    if len(pdf_files) > 10:
                                        st.markdown(f"*... and {len(pdf_files) - 10} more files*")
                            else:
                                st.error(f"âŒ No PDF files found in {pdf_folder}")
                                st.info("ğŸ’¡ Make sure your folder contains .pdf files")
                    else:
                        st.error(f"âŒ Folder does not exist: {pdf_folder}")
                        st.info("ğŸ’¡ Please check the folder path and try again")

            # Show create button if PDFs are found
            if st.session_state.found_pdfs:
                st.markdown('<div style="margin-top: 1.5rem;"></div>', unsafe_allow_html=True)
                
                st.markdown(f"""
                <div class="glass-panel" style="background: rgba(16, 185, 129, 0.1); border-color: rgba(16, 185, 129, 0.3);">
                    <h4 style="color: #10b981; margin-bottom: 0.5rem;">âœ… Ready to Process</h4>
                    <p style="color: rgba(255,255,255,0.8); margin: 0;">
                        <strong>{len(st.session_state.found_pdfs)} PDF files</strong> will be processed from <strong>{st.session_state.pdf_folder}</strong>
                    </p>
                    <p style="color: rgba(255,255,255,0.6); margin: 0.5rem 0 0 0; font-size: 0.9rem;">
                        This will create text chunks and vector embeddings for intelligent search
                    </p>
                </div>
                """, unsafe_allow_html=True)

                if st.button("ğŸš€ Create Vector Database", key="create_db", width="stretch"):
                    result = create_new_database(
                        st.session_state.found_pdfs,
                        st.session_state.save_folder
                    )

                    if result:
                        vector_store, bm25_retriever, documents, embeddings, metadata = result
                        st.session_state.vector_store = vector_store
                        st.session_state.bm25_retriever = bm25_retriever
                        st.session_state.documents = documents
                        st.session_state.embeddings = embeddings
                        st.session_state.metadata = metadata
                        st.session_state.database_loaded = True
                        st.session_state.found_pdfs = []  # Reset
                        st.success("âœ… Database created successfully! Switch to AI Model tab to configure.")
                        st.balloons()

    if not st.session_state.database_loaded:
        st.info("ğŸ‘† Please load documents first")
    else:
        model_provider = st.selectbox(
            "ğŸ­ Model Provider",
            ["API Models (Groq)", "Local Models (Ollama)"]
        )
        
        if model_provider == "API Models (Groq)":
            if groq_api_key:
                st.success("âœ… Groq API Key configured")
                selected_model_display = st.selectbox(
                    "Select Model",
                    list(AVAILABLE_MODELS["API Models (Groq)"].keys())
                )
                selected_model = AVAILABLE_MODELS["API Models (Groq)"][selected_model_display]
            else:
                st.error("âŒ No API key found. Add GROQ_API_KEY to Streamlit Secrets.")
                selected_model = None
        else:
            ollama_running = check_ollama_connection()
            if ollama_running:
                available_models = get_available_ollama_models()
                if available_models:
                    st.success("âœ… Ollama is running")
                    selected_model = st.selectbox("Select Model", available_models)
                else:
                    st.error("âŒ No models found. Run: `ollama pull llama3.2:3b`")
                    selected_model = None
            else:
                st.error("âŒ Ollama is not running")
                selected_model = None
        
        if selected_model and st.button("ğŸš€ Activate AI", key="init_model", use_container_width=True):
            with st.spinner("Initializing AI..."):
                llm = initialize_llm(model_provider, selected_model, groq_api_key)
                
                if llm:
                    st.session_state.llm = llm
                    st.session_state.model_provider = model_provider
                    st.session_state.selected_model = selected_model
                    st.session_state.llm_configured = True
                    st.success("âœ… AI Ready! Go to Chat tab.")
                    st.balloons()

# TAB 3: CHAT
with tab3:
    if not st.session_state.database_loaded or not st.session_state.llm_configured:
        st.markdown("""
        <div class="modern-card">
            <h3 style="color: white; text-align: center;">âš ï¸ Configuration Required</h3>
            <p style="color: rgba(255,255,255,0.7); text-align: center; margin-top: 1rem;">
                Please configure your database and AI model in the Configuration tab before starting a chat.
            </p>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown('<div class="card-title"><span class="card-title-icon">ğŸ’¬</span>Intelligent Conversation</div>', unsafe_allow_html=True)

        # Create hybrid retriever
        hybrid_retriever = create_hybrid_retriever(
            st.session_state.vector_store,
            st.session_state.bm25_retriever
        )

        llm = st.session_state.llm

        prompt = ChatPromptTemplate.from_template("""
        You are an expert assistant analyzing documents. Answer the following question based only on the provided context.
        Be comprehensive, detailed, and include specific references.

        <context>
        {context}
        </context>

        Question: {input}

        Answer:
        """)

        document_chain = create_stuff_documents_chain(llm, prompt)
        retrieval_chain = create_retrieval_chain(hybrid_retriever, document_chain)

        # Display chat history
        for idx, message in enumerate(st.session_state.messages):
            with st.chat_message(message["role"]):
                if message["role"] == "assistant":
                    st.markdown(message["content"], unsafe_allow_html=True)

                    # Display sources if available
                    if "sources" in message and message["sources"]:
                        with st.expander("ğŸ“š View Sources", expanded=False):
                            import pandas as pd
                            # Create simplified dataframe for display
                            display_sources = []
                            for s in message["sources"]:
                                display_sources.append({
                                    "Citation": s.get("Citation", ""),
                                    "Document": s.get("Document", "Unknown"),
                                    "Page": str(s.get("Page", "Unknown"))
                                })
                            df = pd.DataFrame(display_sources)
                            st.dataframe(df, use_container_width=True, hide_index=True)

                            # Show detailed source content
                            st.markdown("---")
                            st.markdown("**ğŸ“– Source Content Preview:**")
                            for i, source in enumerate(message["sources"]):
                                st.markdown(f"**[{i+1}] {source.get('Document', 'Unknown')} - Page {source.get('Page', 'Unknown')}**")
                                if "content" in source:
                                    preview = source["content"][:300] + "..." if len(source["content"]) > 300 else source["content"]
                                    st.markdown(f'<div style="background: rgba(255,255,255,0.03); padding: 1rem; border-radius: 8px; margin: 0.5rem 0 1rem 0; color: rgba(255,255,255,0.7); line-height: 1.6;">{preview}</div>', unsafe_allow_html=True)
                else:
                    st.markdown(message["content"])

        # Chat input at the bottom
        if prompt_input := st.chat_input("ğŸ’­ Ask me anything about your documents..."):
            # Add user message
            st.session_state.messages.append({"role": "user", "content": prompt_input})

            # Generate response
            try:
                response = retrieval_chain.invoke({"input": prompt_input})
                answer = response["answer"]

                # Process answer with citations
                processed_answer = process_answer_with_citations(answer, response["context"])
                styled_answer = f'<div class="answer-content">{processed_answer}</div>'

                # Prepare source data
                import pandas as pd
                source_data = []
                for i, doc in enumerate(response["context"]):
                    source_file = doc.metadata.get('source_file', 'Unknown')
                    page_num = doc.metadata.get('page', 'Unknown')
                    full_path = doc.metadata.get('full_path', '')
                    source_data.append({
                        "Citation": f"[{i+1}]",
                        "Document": source_file,
                        "Page": str(page_num),
                        "content": doc.page_content,
                        "path": full_path
                    })

                # Save to session with sources
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": styled_answer,
                    "sources": source_data
                })

                # Force rerun to display the new message
                st.rerun()

            except Exception as e:
                error_msg = f"âŒ Error: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})
                st.rerun()

        # Action buttons
        st.markdown('<div style="margin-top: 2rem;"></div>', unsafe_allow_html=True)
        col1, col2, col3 = st.columns([1, 1, 1])
        with col1:
            if st.button("ğŸ—‘ï¸ Clear Chat", key="clear_chat"):
                st.session_state.messages = []
                st.rerun()
        with col2:
            if st.button("ğŸ”„ Reset All", key="reset_config"):
                st.session_state.database_loaded = False
                st.session_state.llm_configured = False
                st.session_state.messages = []
                st.session_state.scanned_databases = []
                st.session_state.found_pdfs = []
                st.rerun()

st.markdown('</div>', unsafe_allow_html=True)

# Footer with Copyright
st.markdown("""
<div style="
    text-align: center;
    padding: 2rem 0 1rem 0;
    margin-top: 3rem;
    border-top: 1px solid rgba(255, 255, 255, 0.1);">
    <p style="color: rgba(255, 255, 255, 0.5); font-size: 0.875rem; margin: 0;">
        Â© 2025 DocuMind AI. All rights reserved. | Powered by Advanced RAG Technology
    </p>
</div>
""", unsafe_allow_html=True)
